#!/usr/bin/env python3
"""
ë¹„ë””ì˜¤ í”„ë ˆì„ ì¶”ì¶œ ë° Detection í…ŒìŠ¤íŠ¸
people.mp4ì˜ íŠ¹ì • í”„ë ˆì„ë“¤ì„ ì¶”ì¶œí•´ì„œ ê°œë³„ì ìœ¼ë¡œ í…ŒìŠ¤íŠ¸
"""

import sys
import time
from pathlib import Path

import cv2

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from src.tracking.detector_configs import get_config
from src.tracking.yolox_detector import YOLOXDetector


def extract_and_test_frames(video_path: str, frame_indices=[10, 50, 100, 200, 300]):
    """ë¹„ë””ì˜¤ì—ì„œ íŠ¹ì • í”„ë ˆì„ë“¤ì„ ì¶”ì¶œí•´ì„œ detection í…ŒìŠ¤íŠ¸"""

    print(f"ğŸ¬ ë¹„ë””ì˜¤ í”„ë ˆì„ ì¶”ì¶œ í…ŒìŠ¤íŠ¸: {video_path}")

    # ë¹„ë””ì˜¤ ì—´ê¸°
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        print(f"âŒ ë¹„ë””ì˜¤ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {video_path}")
        return

    fps = cap.get(cv2.CAP_PROP_FPS)
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

    print(f"ğŸ“Š ë¹„ë””ì˜¤ ì •ë³´: {width}x{height}, {fps}fps, ì´ {frame_count}í”„ë ˆì„")

    # Detector ì´ˆê¸°í™”
    config = get_config("crowded_scene")
    detector = YOLOXDetector(
        model_name=config.model_name,
        confidence_threshold=config.confidence_threshold,
        enable_image_enhancement=True,
        nms_iou_threshold=0.3,
    )
    print(
        f"ğŸ¯ Detector ì´ˆê¸°í™”: {config.model_name}, ì„ê³„ê°’: {config.confidence_threshold}"
    )

    # ê° í”„ë ˆì„ì— ëŒ€í•´ í…ŒìŠ¤íŠ¸
    for frame_idx in frame_indices:
        if frame_idx >= frame_count:
            print(f"âš ï¸  í”„ë ˆì„ {frame_idx}ëŠ” ë²”ìœ„ë¥¼ ë²—ì–´ë‚¨ (ìµœëŒ€: {frame_count})")
            continue

        print(f"\nğŸ” í”„ë ˆì„ {frame_idx} í…ŒìŠ¤íŠ¸ ì¤‘...")

        # íŠ¹ì • í”„ë ˆì„ìœ¼ë¡œ ì´ë™
        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
        ret, frame = cap.read()

        if not ret:
            print(f"âŒ í”„ë ˆì„ {frame_idx}ë¥¼ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            continue

        print(f"ğŸ“ í”„ë ˆì„ í¬ê¸°: {frame.shape}")

        # Detection ìˆ˜í–‰
        start_time = time.time()
        detections = detector.detect_objects(frame)
        detection_time = time.time() - start_time

        print(f"â±ï¸  Detection ì‹œê°„: {detection_time:.3f}ì´ˆ")
        print(f"ğŸ‘ï¸  ê°ì§€ëœ ê°ì²´ ìˆ˜: {len(detections)}")

        # ì‹ ë¢°ë„ë³„ ë¶„ë¥˜
        high_conf = [d for d in detections if d.confidence >= 0.7]
        medium_conf = [d for d in detections if 0.4 <= d.confidence < 0.7]
        low_conf = [d for d in detections if d.confidence < 0.4]

        print(
            f"ğŸ“Š ì‹ ë¢°ë„ë³„ ë¶„ë¥˜: ë†’ìŒ({len(high_conf)}) ì¤‘ê°„({len(medium_conf)}) ë‚®ìŒ({len(low_conf)})"
        )

        # ë°”ìš´ë”© ë°•ìŠ¤ ìœ íš¨ì„± ì²´í¬
        img_h, img_w = frame.shape[:2]
        valid_count = 0
        invalid_count = 0

        for det in detections:
            x, y, w, h = det.bbox
            if (
                x >= 0
                and y >= 0
                and x + w <= img_w
                and y + h <= img_h
                and w > 0
                and h > 0
            ):
                valid_count += 1
            else:
                invalid_count += 1

        print(f"ğŸ” ë°”ìš´ë”© ë°•ìŠ¤: ìœ íš¨({valid_count}) ë¬´íš¨({invalid_count})")

        # ê°ì§€ëœ ê°ì²´ ì •ë³´ ì¶œë ¥ (ìƒìœ„ 10ê°œë§Œ)
        detections_sorted = sorted(detections, key=lambda x: x.confidence, reverse=True)
        print("ğŸ“‹ ìƒìœ„ 10ê°œ ê°ì§€ ê²°ê³¼:")
        for i, det in enumerate(detections_sorted[:10]):
            print(
                f"   {i+1:2d}. ì‹ ë¢°ë„: {det.confidence:.3f}, í¬ê¸°: {det.bbox[2]:3d}x{det.bbox[3]:3d}, ìœ„ì¹˜: ({det.bbox[0]:4d}, {det.bbox[1]:4d})"
            )

        # ê²°ê³¼ ì´ë¯¸ì§€ ì €ì¥
        annotated_frame = frame.copy()
        for det in detections:
            x, y, w, h = det.bbox
            cv2.rectangle(annotated_frame, (x, y), (x + w, y + h), (0, 255, 0), 2)

            label = f"{det.class_name}: {det.confidence:.2f}"
            cv2.putText(
                annotated_frame,
                label,
                (x, y - 10),
                cv2.FONT_HERSHEY_SIMPLEX,
                0.5,
                (0, 255, 0),
                2,
            )

        # í”„ë ˆì„ ì •ë³´ ì¶”ê°€
        info_text = (
            f"Frame {frame_idx}: {len(detections)} detections in {detection_time:.3f}s"
        )
        cv2.putText(
            annotated_frame,
            info_text,
            (10, 30),
            cv2.FONT_HERSHEY_SIMPLEX,
            0.7,
            (255, 255, 255),
            2,
        )

        # ê²°ê³¼ ì €ì¥
        output_path = f"temp/debug_frame_{frame_idx}_result.jpg"
        cv2.imwrite(output_path, annotated_frame)
        print(f"ğŸ’¾ ê²°ê³¼ ì €ì¥: {output_path}")

    cap.release()
    print("\nâœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")


def compare_continuous_vs_jump_frames(video_path: str):
    """ì—°ì† í”„ë ˆì„ vs ì í”„ í”„ë ˆì„ ë¹„êµ"""

    print("ğŸ”„ ì—°ì† vs ì í”„ í”„ë ˆì„ ë¹„êµ í…ŒìŠ¤íŠ¸")

    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        print("âŒ ë¹„ë””ì˜¤ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        return

    # Detector ì´ˆê¸°í™”
    config = get_config("crowded_scene")
    detector = YOLOXDetector(
        model_name=config.model_name,
        confidence_threshold=config.confidence_threshold,
        enable_image_enhancement=True,
    )

    print("\nğŸ“¹ ì—°ì† í”„ë ˆì„ ì½ê¸° í…ŒìŠ¤íŠ¸ (í”„ë ˆì„ 100-110)")
    total_detections_continuous = 0

    # ì—°ì†ìœ¼ë¡œ í”„ë ˆì„ ì½ê¸°
    cap.set(cv2.CAP_PROP_POS_FRAMES, 100)
    for i in range(10):
        ret, frame = cap.read()
        if not ret:
            break

        detections = detector.detect_objects(frame)
        total_detections_continuous += len(detections)
        print(f"   í”„ë ˆì„ {100+i}: {len(detections)}ê°œ ê°ì§€")

    print(f"ğŸ“Š ì—°ì† ì½ê¸° ì´ ê°ì§€: {total_detections_continuous}ê°œ")

    print("\nğŸ¦˜ ì í”„ í”„ë ˆì„ ì½ê¸° í…ŒìŠ¤íŠ¸ (100, 200, 300...)")
    total_detections_jump = 0

    # ì í”„í•´ì„œ í”„ë ˆì„ ì½ê¸°
    for frame_idx in [100, 200, 300, 400, 500]:
        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
        ret, frame = cap.read()
        if not ret:
            continue

        detections = detector.detect_objects(frame)
        total_detections_jump += len(detections)
        print(f"   í”„ë ˆì„ {frame_idx}: {len(detections)}ê°œ ê°ì§€")

    print(f"ğŸ“Š ì í”„ ì½ê¸° ì´ ê°ì§€: {total_detections_jump}ê°œ")

    cap.release()


if __name__ == "__main__":
    video_path = "data/people.mp4"

    if not Path(video_path).exists():
        print(f"âŒ ë¹„ë””ì˜¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {video_path}")
        print("ğŸ“ data ë””ë ‰í† ë¦¬ì˜ íŒŒì¼ë“¤:")
        for f in Path("data").glob("*.mp4"):
            print(f"   {f}")
        exit(1)

    print("ğŸš€ ë¹„ë””ì˜¤ í”„ë ˆì„ ë””ë²„ê¹… ì‹œì‘")
    print("=" * 50)

    # temp ë””ë ‰í† ë¦¬ ìƒì„±
    Path("temp").mkdir(exist_ok=True)

    # 1. íŠ¹ì • í”„ë ˆì„ë“¤ ì¶”ì¶œí•´ì„œ í…ŒìŠ¤íŠ¸
    extract_and_test_frames(video_path, [50, 100, 200, 287, 400])

    print("\n" + "=" * 50)

    # 2. ì—°ì† vs ì í”„ í”„ë ˆì„ ë¹„êµ
    compare_continuous_vs_jump_frames(video_path)

    print("\nğŸ¯ ê²°ë¡ :")
    print("1. temp/debug_frame_*.jpg íŒŒì¼ë“¤ì„ í™•ì¸í•´ë³´ì„¸ìš”")
    print("2. ì—°ì† ì½ê¸° vs ì í”„ ì½ê¸° ê²°ê³¼ë¥¼ ë¹„êµí•´ë³´ì„¸ìš”")
    print("3. Streamlitì—ì„œëŠ” ì—°ì† ì½ê¸°ë¥¼ í•˜ë¯€ë¡œ ì°¨ì´ê°€ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤")

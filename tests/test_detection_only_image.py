#!/usr/bin/env python3
"""
Detectionë§Œ í…ŒìŠ¤íŠ¸í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸ - data/people_1.png ê³ ì • í…ŒìŠ¤íŠ¸
Streamlit ì—†ì´ OpenCVë¡œ ë¹ ë¥´ê²Œ í™•ì¸
"""

import sys
import time
from pathlib import Path

import cv2

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from src.tracking.detector_configs import get_config
from src.tracking.yolox_detector import YOLOXDetector


def test_detection_on_image_fixed():
    """data/people_1.pngì—ì„œ ê³ ì • detection í…ŒìŠ¤íŠ¸"""

    print("ğŸ–¼ï¸  ê³ ì • ì´ë¯¸ì§€ Detection í…ŒìŠ¤íŠ¸ (data/people_1.png)")
    print("=" * 60)

    # ì´ë¯¸ì§€ ê²½ë¡œ ê³ ì •
    image_path = "data/people_1.png"

    if not Path(image_path).exists():
        print(f"âŒ ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {image_path}")
        return

    # í˜„ì¬ ì„¤ì •ìœ¼ë¡œ í…ŒìŠ¤íŠ¸ (crowded_sceneê³¼ ë™ì¼)
    config = get_config("crowded_scene")
    detector = YOLOXDetector(
        model_name=config.model_name,
        confidence_threshold=config.confidence_threshold,  # 0.25
        enable_image_enhancement=False,
        nms_iou_threshold=0.4,
    )

    # ì´ë¯¸ì§€ ë¡œë“œ
    image = cv2.imread(image_path)
    if image is None:
        print(f"âŒ ì´ë¯¸ì§€ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {image_path}")
        return

    print(f"ğŸ“Š ì´ë¯¸ì§€ í¬ê¸°: {image.shape}")
    print(f"ğŸ”§ ì‚¬ìš© ì„¤ì •: {config.model_name}, ì„ê³„ê°’: {config.confidence_threshold}")

    # Detection ìˆ˜í–‰
    start_time = time.time()
    detections = detector.detect_objects(image)
    detection_time = time.time() - start_time

    print(f"â±ï¸  Detection ì‹œê°„: {detection_time:.3f}ì´ˆ")
    print(f"ğŸ‘ï¸  ê°ì§€ëœ ê°ì²´ ìˆ˜: {len(detections)}")

    # ì‹ ë¢°ë„ë³„ ë¶„ë¥˜
    high_conf = [d for d in detections if d.confidence >= 0.7]
    medium_conf = [d for d in detections if 0.4 <= d.confidence < 0.7]
    low_conf = [d for d in detections if d.confidence < 0.4]

    print(f"ğŸ“Š ì‹ ë¢°ë„ë³„ ë¶„ë¥˜:")
    print(f"   ë†’ìŒ (â‰¥0.7): {len(high_conf)}ê°œ")
    print(f"   ì¤‘ê°„ (0.4~0.7): {len(medium_conf)}ê°œ")
    print(f"   ë‚®ìŒ (<0.4): {len(low_conf)}ê°œ")

    # ëª¨ë“  ê°ì§€ ê²°ê³¼ í‘œì‹œ (ìƒìœ„ 20ê°œ)
    detections_sorted = sorted(detections, key=lambda x: x.confidence, reverse=True)
    print("\nğŸ“‹ ìƒìœ„ 20ê°œ ê°ì§€ ê²°ê³¼:")
    for i, det in enumerate(detections_sorted[:20]):
        print(
            f"   {i+1:2d}. ì‹ ë¢°ë„: {det.confidence:.3f}, í¬ê¸°: {det.bbox[2]:3d}x{det.bbox[3]:3d}, ìœ„ì¹˜: ({det.bbox[0]:4d}, {det.bbox[1]:4d})"
        )

    # ë°”ìš´ë”© ë°•ìŠ¤ ìœ íš¨ì„± ì²´í¬
    img_h, img_w = image.shape[:2]
    valid_detections = []
    invalid_detections = []

    for det in detections:
        x, y, w, h = det.bbox
        x1, y1, x2, y2 = x, y, x + w, y + h

        # ì´ë¯¸ì§€ ê²½ê³„ ì²´í¬
        if x1 >= 0 and y1 >= 0 and x2 <= img_w and y2 <= img_h and w > 0 and h > 0:
            valid_detections.append(det)
        else:
            invalid_detections.append(det)

    print(f"\nğŸ” ë°”ìš´ë”© ë°•ìŠ¤ ìœ íš¨ì„± ì²´í¬:")
    print(f"   ìœ íš¨í•œ ë°•ìŠ¤: {len(valid_detections)}ê°œ")
    print(f"   ë¬´íš¨í•œ ë°•ìŠ¤: {len(invalid_detections)}ê°œ")

    if invalid_detections:
        print("âŒ ë¬´íš¨í•œ ë°•ìŠ¤ ì˜ˆì‹œ:")
        for i, det in enumerate(invalid_detections[:5]):
            x, y, w, h = det.bbox
            print(
                f"   {i+1}. ìœ„ì¹˜: ({x}, {y}), í¬ê¸°: {w}x{h}, ì‹ ë¢°ë„: {det.confidence:.3f}"
            )

    # ê²°ê³¼ë¥¼ ì´ë¯¸ì§€ì— ê·¸ë¦¬ê¸° - ìœ íš¨í•œ ê°ì§€ ê²°ê³¼ë§Œ
    result_image = image.copy()

    print(f"\nğŸ¨ {len(valid_detections)}ê°œ ìœ íš¨í•œ ê°ì§€ ê²°ê³¼ë¥¼ ì‹œê°í™” ì¤‘...")

    drawn_count = 0
    for i, det in enumerate(valid_detections):
        x, y, w, h = det.bbox

        # ë‹¤ì‹œ í•œë²ˆ ê²½ê³„ ì²´í¬
        if x < 0 or y < 0 or x + w > img_w or y + h > img_h:
            continue

        # ì‹ ë¢°ë„ì— ë”°ë¥¸ ìƒ‰ìƒê³¼ ë‘ê»˜
        if det.confidence >= 0.7:
            color = (0, 255, 0)  # ì´ˆë¡ìƒ‰ - ë†’ì€ ì‹ ë¢°ë„
            thickness = 3
        elif det.confidence >= 0.4:
            color = (0, 255, 255)  # ë…¸ë€ìƒ‰ - ì¤‘ê°„ ì‹ ë¢°ë„
            thickness = 2
        else:
            color = (0, 0, 255)  # ë¹¨ê°„ìƒ‰ - ë‚®ì€ ì‹ ë¢°ë„
            thickness = 1

        # ë°”ìš´ë”© ë°•ìŠ¤ ê·¸ë¦¬ê¸°
        try:
            cv2.rectangle(
                result_image,
                (int(x), int(y)),
                (int(x + w), int(y + h)),
                color,
                thickness,
            )
            drawn_count += 1

            # ì‹ ë¢°ë„ ë¼ë²¨ (ì‘ê²Œ)
            label = f"{det.confidence:.2f}"
            font_scale = 0.4
            font_thickness = 1

            # ë¼ë²¨ í¬ê¸° ê³„ì‚°
            (label_w, label_h), baseline = cv2.getTextSize(
                label, cv2.FONT_HERSHEY_SIMPLEX, font_scale, font_thickness
            )

            # ë¼ë²¨ ìœ„ì¹˜ ì¡°ì • (ì´ë¯¸ì§€ ê²½ê³„ ê³ ë ¤)
            label_x = max(0, min(int(x), img_w - label_w))
            label_y = max(label_h + 5, int(y))

            # ë¼ë²¨ ë°°ê²½
            cv2.rectangle(
                result_image,
                (label_x, label_y - label_h - 2),
                (label_x + label_w, label_y + 2),
                color,
                -1,
            )

            # ë¼ë²¨ í…ìŠ¤íŠ¸
            cv2.putText(
                result_image,
                label,
                (label_x, label_y),
                cv2.FONT_HERSHEY_SIMPLEX,
                font_scale,
                (0, 0, 0),
                font_thickness,
            )

        except Exception as e:
            print(f"âš ï¸  ë°”ìš´ë”© ë°•ìŠ¤ ê·¸ë¦¬ê¸° ì‹¤íŒ¨ (ì¸ë±ìŠ¤ {i}): {e}")
            continue

    print(f"âœ… ì‹¤ì œë¡œ ê·¸ë ¤ì§„ ë°”ìš´ë”© ë°•ìŠ¤: {drawn_count}ê°œ")

    # ì •ë³´ í…ìŠ¤íŠ¸ ì¶”ê°€
    info_texts = [
        f"Total detections: {len(detections)}",
        f"Valid detections: {len(valid_detections)}",
        f"Drawn boxes: {drawn_count}",
        f"High conf (>=0.7): {len(high_conf)}",
        f"Medium conf (0.4-0.7): {len(medium_conf)}",
        f"Low conf (<0.4): {len(low_conf)}",
    ]

    for i, text in enumerate(info_texts):
        cv2.putText(
            result_image,
            text,
            (10, 25 + i * 20),
            cv2.FONT_HERSHEY_SIMPLEX,
            0.5,
            (255, 255, 255),
            1,
            cv2.LINE_AA,
        )

    # ë²”ë¡€ ì¶”ê°€ (ë” ì•„ë˜ìª½ìœ¼ë¡œ)
    legend_y = img_h - 100
    cv2.putText(
        result_image,
        "Legend:",
        (10, legend_y),
        cv2.FONT_HERSHEY_SIMPLEX,
        0.4,
        (255, 255, 255),
        1,
    )

    # ë²”ë¡€ ë°•ìŠ¤ë“¤
    legend_items = [
        ("High conf (>=0.7)", (0, 255, 0)),
        ("Medium conf (0.4-0.7)", (0, 255, 255)),
        ("Low conf (<0.4)", (0, 0, 255)),
    ]

    for i, (text, color) in enumerate(legend_items):
        y_pos = legend_y + 15 + i * 20
        cv2.rectangle(result_image, (10, y_pos), (25, y_pos + 10), color, -1)
        cv2.putText(
            result_image,
            text,
            (30, y_pos + 8),
            cv2.FONT_HERSHEY_SIMPLEX,
            0.3,
            (255, 255, 255),
            1,
        )

    # ê²°ê³¼ ì €ì¥
    output_path = "temp/detection_result_debug.jpg"
    cv2.imwrite(output_path, result_image)
    print(f"ğŸ’¾ ê²°ê³¼ ì´ë¯¸ì§€ ì €ì¥: {output_path}")

    # í™”ë©´ì— í‘œì‹œ
    cv2.imshow("Debug - All Detections", result_image)
    print("ğŸ–±ï¸  ì•„ë¬´ í‚¤ë‚˜ ëˆ„ë¥´ë©´ ì¢…ë£Œ...")
    cv2.waitKey(0)
    cv2.destroyAllWindows()


def test_detection_on_video(video_path: str):
    """ë¹„ë””ì˜¤ì—ì„œ detection í…ŒìŠ¤íŠ¸"""

    print("ğŸ¯ Detection í…ŒìŠ¤íŠ¸ ì‹œì‘...")

    # 1. YOLOXDetector ì´ˆê¸°í™” - ê°œì„ ëœ ì„¤ì •
    print("ğŸ“¡ YOLOX Detector ì´ˆê¸°í™” ì¤‘...")
    try:
        detector = YOLOXDetector(
            model_name="fasterrcnn_resnet50_fpn",
            confidence_threshold=0.7,  # ë” ë†’ì€ ì„ê³„ê°’
            enable_image_enhancement=False,  # ë¹„ë””ì˜¤ì—ì„œëŠ” ë¹„í™œì„±í™”
            nms_iou_threshold=0.4,  # ë” ì—„ê²©í•œ NMS
        )
        print(f"âœ… Detector ì´ˆê¸°í™” ì™„ë£Œ")
    except Exception as e:
        print(f"âŒ Detector ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return

    # 2. ë¹„ë””ì˜¤ ì—´ê¸°
    print(f"ğŸ“¹ ë¹„ë””ì˜¤ ì—´ê¸°: {video_path}")
    cap = cv2.VideoCapture(video_path)

    if not cap.isOpened():
        print(f"âŒ ë¹„ë””ì˜¤ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {video_path}")
        return

    fps = cap.get(cv2.CAP_PROP_FPS)
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

    print(f"ğŸ“Š ë¹„ë””ì˜¤ ì •ë³´: {width}x{height}, {fps}fps, {frame_count}í”„ë ˆì„")

    # 3. í”„ë ˆì„ë³„ detection í…ŒìŠ¤íŠ¸
    frame_num = 0
    total_detections = 0
    start_time = time.time()

    while True:
        ret, frame = cap.read()
        if not ret:
            break

        frame_num += 1

        # ë§¤ 10ë²ˆì§¸ í”„ë ˆì„ë§Œ ì²˜ë¦¬ (ì†ë„ í–¥ìƒ)
        if frame_num % 10 != 0:
            continue

        print(f"\nğŸ” í”„ë ˆì„ {frame_num} ì²˜ë¦¬ ì¤‘...")

        # Detection ìˆ˜í–‰
        detection_start = time.time()
        detections = detector.detect_objects(frame)
        detection_time = time.time() - detection_start

        print(f"â±ï¸  Detection ì‹œê°„: {detection_time:.3f}ì´ˆ")
        print(f"ğŸ‘ï¸  ê°ì§€ëœ ê°ì²´ ìˆ˜: {len(detections)}")

        # ê°ì§€ëœ ê°ì²´ ì •ë³´ ì¶œë ¥
        for i, det in enumerate(detections):
            print(
                f"   {i+1}. ID: {det.track_id}, í´ë˜ìŠ¤: {det.class_name}, "
                f"ì‹ ë¢°ë„: {det.confidence:.3f}, ë°”ìš´ë”©ë°•ìŠ¤: {det.bbox}"
            )

        total_detections += len(detections)

        # ê²°ê³¼ë¥¼ í”„ë ˆì„ì— ê·¸ë¦¬ê¸°
        annotated_frame = frame.copy()
        for det in detections:
            x, y, w, h = det.bbox
            cv2.rectangle(annotated_frame, (x, y), (x + w, y + h), (0, 255, 0), 2)

            # ë¼ë²¨ í‘œì‹œ
            label = f"{det.class_name}: {det.confidence:.2f}"
            cv2.putText(
                annotated_frame,
                label,
                (x, y - 10),
                cv2.FONT_HERSHEY_SIMPLEX,
                0.5,
                (0, 255, 0),
                2,
            )

        # ì •ë³´ í‘œì‹œ
        info_text = f"Frame: {frame_num}, Detections: {len(detections)}, Time: {detection_time:.3f}s"
        cv2.putText(
            annotated_frame,
            info_text,
            (10, 30),
            cv2.FONT_HERSHEY_SIMPLEX,
            0.7,
            (255, 255, 255),
            2,
        )

        # í™”ë©´ì— í‘œì‹œ
        cv2.imshow("Detection Test", annotated_frame)

        # ESC í‚¤ë¡œ ì¢…ë£Œ
        key = cv2.waitKey(1) & 0xFF
        if key == 27:  # ESC
            print("\nâ¹ï¸  ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨")
            break

        # ì²˜ìŒ 50í”„ë ˆì„ë§Œ ì²˜ë¦¬ (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸)
        if frame_num >= 50:
            print(f"\nâ¹ï¸  í…ŒìŠ¤íŠ¸ ì™„ë£Œ (50í”„ë ˆì„ ì²˜ë¦¬)")
            break

    # ê²°ê³¼ ìš”ì•½
    elapsed_time = time.time() - start_time
    avg_detections = total_detections / (frame_num // 10) if frame_num > 0 else 0

    print(f"\nğŸ“ˆ í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
    print(f"   ì²˜ë¦¬ëœ í”„ë ˆì„: {frame_num // 10}")
    print(f"   ì´ ì†Œìš” ì‹œê°„: {elapsed_time:.2f}ì´ˆ")
    print(f"   í‰ê·  ì´ˆë‹¹ í”„ë ˆì„: {(frame_num // 10) / elapsed_time:.2f} FPS")
    print(f"   ì´ ê°ì§€ ê°ì²´: {total_detections}")
    print(f"   í‰ê·  í”„ë ˆì„ë‹¹ ê°ì§€: {avg_detections:.1f}ê°œ")

    cap.release()
    cv2.destroyAllWindows()


if __name__ == "__main__":
    print("ğŸš€ YOLOX Detection ë””ë²„ê·¸ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("=" * 50)

    # ê³ ì • ì´ë¯¸ì§€ í…ŒìŠ¤íŠ¸ë§Œ ì‹¤í–‰
    test_detection_on_image_fixed()
